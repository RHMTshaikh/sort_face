{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a69071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "from sklearn.cluster import DBSCAN\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paths\n",
    "\n",
    "model_name = 'Facenet512' # You can also try 'VGG-Face', 'OpenFace', 'DeepFace', etc.\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR = paths.INPUT_DIR # Folder containing the original photos\n",
    "last_folder_name = os.path.basename(INPUT_DIR)\n",
    "OUTPUT_DIR = f\"results/{last_folder_name}_{model_name}\" # Folder to save the final results\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# List of valid image extensions\n",
    "IMAGE_EXTENSIONS = ('.png', '.jpg', '.jpeg', '.webp', '.bmp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb4ccf",
   "metadata": {},
   "source": [
    "| Model Name | Embedding Length (Dimensions) |\n",
    "| :--- | :--- |\n",
    "| VGG-Face | 2622 |\n",
    "| FaceNet | 128 |\n",
    "| FaceNet512 | 512 |\n",
    "| OpenFace | 128 |\n",
    "| DeepFace | 4096 |\n",
    "| DeepID | 160 |\n",
    "| ArcFace | 512 |\n",
    "| SFace | 128 |\n",
    "| GhostFaceNet | 512 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23085854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_face_data(directory):\n",
    "    \"\"\"\n",
    "    Scans the directory, detects ALL faces in ALL images, and generates embeddings.\n",
    "    \n",
    "    Crucial: DeepFace.represent() returns a list of dictionaries for ALL faces detected in an image.\n",
    "    We need to flatten this into a single list of (embedding, path) pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ Starting face detection and embedding generation...\")\n",
    "    \n",
    "    filenames = os.listdir(directory)\n",
    "    print(f'---Total files {len(filenames)}---')\n",
    "    \n",
    "    face_data = []\n",
    "    \n",
    "    for idx, filename in enumerate(filenames):\n",
    "        print(idx, end='')\n",
    "        if not filename.lower().endswith(IMAGE_EXTENSIONS):\n",
    "            continue\n",
    "            \n",
    "        path = os.path.join(directory, filename)\n",
    "        \n",
    "        try:\n",
    "            results = DeepFace.represent(\n",
    "                img_path=path, \n",
    "                model_name=model_name, \n",
    "                detector_backend=\"yolov12n\",\n",
    "                # l2_normalize=True, # Normalize embeddings to unit length (important for distance-based clustering)\n",
    "                enforce_detection=True # Set to False only if you know every image has a face\n",
    "            )\n",
    "            \n",
    "            # Each 'results' item is a dictionary for ONE detected face in the image\n",
    "            for face_obj in results:\n",
    "                embedding = face_obj['embedding']\n",
    "                x = face_obj['facial_area']['x']\n",
    "                y = face_obj['facial_area']['y']\n",
    "                w = face_obj['facial_area']['w']\n",
    "                h = face_obj['facial_area']['h']\n",
    "                face_location = (x, y, w, h)\n",
    "                face_data.append({\n",
    "                    'embedding': embedding,\n",
    "                    'image_path': path,\n",
    "                    'face_location': face_location\n",
    "                })\n",
    "            \n",
    "            print(f\"  -> Processed {filename} Found {len(results)} face(s).\")\n",
    "        \n",
    "            \n",
    "        except ValueError as e:\n",
    "            # DeepFace raises ValueError if no face is detected\n",
    "            if \"Face could not be detected\" in str(e):\n",
    "                print(f\"  -> No face detected in {filename}. Skipping.\")\n",
    "            else:\n",
    "                print(f\"  -> Error processing {filename}: {e}\")\n",
    "        except Exception as e:\n",
    "             print(f\"  -> Unexpected error processing {filename}: {e}\")\n",
    "    \n",
    "    print(f\"Length of embedding is {len(face_data[0]['embedding'])} and total faces detected: {len(face_data)}\")\n",
    "\n",
    "    return face_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b64f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_sort_photos(all_face_data, eps=1.5):\n",
    "    \"\"\"\n",
    "    Clusters the embeddings and sorts the photos into named folders.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not all_face_data:\n",
    "        print(\"\\nðŸš« No faces found to cluster. Check your 'input_photos' folder.\")\n",
    "        return\n",
    "\n",
    "    # delete all the contents of OUTPUT_DIR before saving new results\n",
    "    for filename in os.listdir(OUTPUT_DIR):\n",
    "        file_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "            \n",
    "    # Extract only the embedding vectors into a NumPy array\n",
    "    embeddings = np.array([d['embedding'] for d in all_face_data])\n",
    "    \n",
    "    print(f\"\\nClustering {len(embeddings)} total face embeddings...\")\n",
    "\n",
    "    # --- DBSCAN Clustering ---\n",
    "    # eps: The max distance for two embeddings to be considered the same person.\n",
    "    # ArcFace (used here) has 512-dimensional embeddings, so the distance is larger \n",
    "    # than the 128-dim vectors from Dlib/face_recognition.\n",
    "    # You might need to tune this value (try 1.0 to 1.5)\n",
    "    \n",
    "    cl = DBSCAN(metric=\"cosine\", n_jobs=-1, eps=eps, min_samples=2).fit(embeddings)\n",
    "    label_ids = cl.labels_\n",
    "    \n",
    "    # Get all unique cluster labels/IDs\n",
    "    unique_labels = np.unique(label_ids)\n",
    "    \n",
    "    # 2. Sorting and Saving\n",
    "    print(\"\\nðŸ“¦ Sorting photos into person folders...\")\n",
    "    \n",
    "    # A set to keep track of which image files have already been copied\n",
    "    copied_files = set()\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        # Check if the label is the noise cluster (-1)\n",
    "        is_noise = (label == -1)\n",
    "        folder_name = \"Unknown_Faces\" if is_noise else f\"Person_{label}\"\n",
    "        person_dir = os.path.join(OUTPUT_DIR, folder_name)\n",
    "        os.makedirs(person_dir, exist_ok=True)\n",
    "        \n",
    "        # Find the indices corresponding to this cluster ID\n",
    "        indices = np.where(label_ids == label)[0]\n",
    "        \n",
    "        # Track the unique image paths that belong to this person/cluster\n",
    "        images_for_this_person = set()\n",
    "        for i in indices:\n",
    "            image_path_and_face_location = (all_face_data[i]['image_path'], all_face_data[i]['face_location'])\n",
    "            images_for_this_person.add((image_path_and_face_location))\n",
    "            # images_for_this_person.add(all_face_data[i]['image_path'])\n",
    "            \n",
    "        print(f\"  -> Folder '{folder_name}' contains {len(images_for_this_person)} unique photos.\")\n",
    "        \n",
    "        # Copy the original image files to the new folder\n",
    "        for src_path, face_location in images_for_this_person:\n",
    "            # We copy the original photo, not the face crop\n",
    "            dst_path = os.path.join(person_dir, os.path.basename(src_path))\n",
    "            \n",
    "            # shutil.copy(src_path, dst_path)\n",
    "\n",
    "            img = cv.imread(src_path)\n",
    "            x, y, w, h = face_location\n",
    "            cv.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2) \n",
    "            cv.imwrite(dst_path, img)\n",
    "\n",
    "    print(\"\\nâœ… Separation complete!\")\n",
    "    print(f\"Results are saved in the '{OUTPUT_DIR}' directory.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import random\n",
    "import cv2 as cv\n",
    "\n",
    "def cluster_and_sort_photos_with_human_feedback(all_face_data, eps=1.5, buffer=0.1):\n",
    "    \"\"\"\n",
    "    Clusters the embeddings and sorts the photos into named folders.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not all_face_data:\n",
    "        print(\"\\nðŸš« No faces found to cluster. Check your 'input_photos' folder.\")\n",
    "        return\n",
    "\n",
    "    # delete all the contents of OUTPUT_DIR before saving new results\n",
    "    for filename in os.listdir(OUTPUT_DIR):\n",
    "        file_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "            \n",
    "    # Extract only the embedding vectors into a NumPy array\n",
    "    embeddings = np.array([d['embedding'] for d in all_face_data])\n",
    "    \n",
    "    print(f\"\\nClustering {len(embeddings)} total face embeddings...\")\n",
    "\n",
    "    # --- DBSCAN Clustering ---\n",
    "    # eps: The max distance for two embeddings to be considered the same person.\n",
    "    # ArcFace (used here) has 512-dimensional embeddings, so the distance is larger \n",
    "    # than the 128-dim vectors from Dlib/face_recognition.\n",
    "    # You might need to tune this value (try 1.0 to 1.5)\n",
    "    metric = \"cosine\"\n",
    "    cl = DBSCAN(metric=metric, n_jobs=-1, eps=eps, min_samples=2).fit(embeddings)\n",
    "    label_ids = cl.labels_\n",
    "    \n",
    "    # Compute the full distance matrix (Cosine is best for faces)\n",
    "    dist_matrix = pairwise_distances(embeddings, metric=metric)\n",
    "    \n",
    "    # Find indices of pairs within the 'Uncertainty Zone'\n",
    "    neighbour_indices = np.where((dist_matrix > (eps)) & \n",
    "                                 (dist_matrix < (eps + buffer)))\n",
    "    \n",
    "    # Filter to avoid self-matches and duplicates (i < j)\n",
    "    candidate_pairs = [(i, j) for i, j in zip(*neighbour_indices) if i < j]\n",
    "    \n",
    "    idx = 0\n",
    "    for i, j in candidate_pairs:\n",
    "        idx += 1\n",
    "        print(f\"\\rQuestion {idx}/{len(candidate_pairs)}\", end=\"\")\n",
    "        \n",
    "        # Only show pairs that are in different clusters (potentially misclassified)\n",
    "        if label_ids[i] == label_ids[j]:\n",
    "            continue\n",
    "        \n",
    "        path1, path2 = all_face_data[i]['image_path'], all_face_data[j]['image_path']\n",
    "        loc1, loc2 = all_face_data[i]['face_location'], all_face_data[j]['face_location']\n",
    "        \n",
    "        # Display side-by-side\n",
    "        img1 = cv.imread(path1)\n",
    "        img2 = cv.imread(path2)\n",
    "        # faces are located at (x, y, w, h)\n",
    "        x1, y1, w1, h1 = loc1\n",
    "        x2, y2, w2, h2 = loc2\n",
    "        # cv.rectangle(img1, (x1, y1), (x1+w1, y1+h1), (0, 255, 0), 2)\n",
    "        # cv.rectangle(img2, (x2, y2), (x2+w2, y2+h2), (0, 255, 0), 2)\n",
    "        face1 = img1[y1:y1+h1, x1:x1+w1]\n",
    "        face2 = img2[y2:y2+h2, x2:x2+w2]\n",
    "        \n",
    "        if h1 < h2:\n",
    "            face1 = cv.resize(face1, (int(w1 * h2 / h1), h2))\n",
    "        else:\n",
    "            face2 = cv.resize(face2, (int(w2 * h1 / h2), h1))\n",
    "        # Resize for display\n",
    "        combined = np.hstack((face1, face2))\n",
    "        \n",
    "        cv.imshow(\"Same Person? (y=Yes, q=Quit)\", combined)\n",
    "        key = cv.waitKey(0) & 0xFF\n",
    "\n",
    "        if key == ord('y'):\n",
    "            label1, label2 = label_ids[i], label_ids[j]\n",
    "            if label1 == -1 and label2 == -1:\n",
    "                # If both are noise, assign a new unique label (max existing + 1)\n",
    "                new_label = max(label_ids) + 1\n",
    "                label_ids[i] = new_label\n",
    "                label_ids[j] = new_label\n",
    "            elif label1 == -1:\n",
    "                label_ids[i] = label2\n",
    "            elif label2 == -1:\n",
    "                label_ids[j] = label1\n",
    "            else:\n",
    "                target_label = min(label1, label2)\n",
    "                collapsed_label = max(label1, label2) \n",
    "                # Update all instances of the collapsed label to the target label\n",
    "                label_ids[label_ids == collapsed_label] = target_label\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv.destroyAllWindows()\n",
    "        \n",
    "    \n",
    "    # Get all unique cluster labels/IDs\n",
    "    unique_labels = np.unique(label_ids)\n",
    "    \n",
    "    # 2. Sorting and Saving\n",
    "    print(\"\\nðŸ“¦ Sorting photos into person folders...\")\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        # Check if the label is the noise cluster (-1)\n",
    "        is_noise = (label == -1)\n",
    "        folder_name = \"Unknown_Faces\" if is_noise else f\"Person_{label}\"\n",
    "        person_dir = os.path.join(OUTPUT_DIR, folder_name)\n",
    "        os.makedirs(person_dir, exist_ok=True)\n",
    "        \n",
    "        # Find the indices corresponding to this cluster ID\n",
    "        indices = np.where(label_ids == label)[0]\n",
    "        \n",
    "        # Track the unique image paths that belong to this person/cluster\n",
    "        images_for_this_person = set()\n",
    "        for i in indices:\n",
    "            image_path_and_face_location = (all_face_data[i]['image_path'], all_face_data[i]['face_location'])\n",
    "            images_for_this_person.add((image_path_and_face_location))\n",
    "            # images_for_this_person.add(all_face_data[i]['image_path'])\n",
    "            \n",
    "        print(f\"  -> Folder '{folder_name}' contains {len(images_for_this_person)} unique photos.\")\n",
    "        \n",
    "        # Copy the original image files to the new folder\n",
    "        for src_path, face_location in images_for_this_person:\n",
    "            # We copy the original photo, not the face crop\n",
    "            dst_path = os.path.join(person_dir, os.path.basename(src_path))\n",
    "            \n",
    "            # shutil.copy(src_path, dst_path)\n",
    "\n",
    "            img = cv.imread(src_path)\n",
    "            x, y, w, h = face_location\n",
    "            cv.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2) \n",
    "            cv.imwrite(dst_path, img)\n",
    "\n",
    "    print(\"\\nâœ… Separation complete!\")\n",
    "    print(f\"Results are saved in the '{OUTPUT_DIR}' directory.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4641de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data and generate encodings\n",
    "all_face_data = get_all_face_data(INPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "83dfa987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15643446504023087"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "epsilon = math.sin(math.radians(9)) # You may need to tune this value based on your dataset and embedding model\n",
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afa880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster and sort\n",
    "cluster_and_sort_photos(all_face_data, eps=epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3253f0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering 1005 total face embeddings...\n",
      "Question 2529/2529\n",
      "ðŸ“¦ Sorting photos into person folders...\n",
      "  -> Folder 'Unknown_Faces' contains 248 unique photos.\n",
      "  -> Folder 'Person_0' contains 14 unique photos.\n",
      "  -> Folder 'Person_1' contains 3 unique photos.\n",
      "  -> Folder 'Person_2' contains 169 unique photos.\n",
      "  -> Folder 'Person_3' contains 31 unique photos.\n",
      "  -> Folder 'Person_4' contains 160 unique photos.\n",
      "  -> Folder 'Person_5' contains 30 unique photos.\n",
      "  -> Folder 'Person_7' contains 2 unique photos.\n",
      "  -> Folder 'Person_8' contains 2 unique photos.\n",
      "  -> Folder 'Person_9' contains 3 unique photos.\n",
      "  -> Folder 'Person_11' contains 2 unique photos.\n",
      "  -> Folder 'Person_12' contains 3 unique photos.\n",
      "  -> Folder 'Person_13' contains 4 unique photos.\n",
      "  -> Folder 'Person_15' contains 12 unique photos.\n",
      "  -> Folder 'Person_16' contains 6 unique photos.\n",
      "  -> Folder 'Person_17' contains 2 unique photos.\n",
      "  -> Folder 'Person_18' contains 3 unique photos.\n",
      "  -> Folder 'Person_19' contains 9 unique photos.\n",
      "  -> Folder 'Person_20' contains 5 unique photos.\n",
      "  -> Folder 'Person_21' contains 9 unique photos.\n",
      "  -> Folder 'Person_22' contains 2 unique photos.\n",
      "  -> Folder 'Person_23' contains 5 unique photos.\n",
      "  -> Folder 'Person_24' contains 2 unique photos.\n",
      "  -> Folder 'Person_25' contains 2 unique photos.\n",
      "  -> Folder 'Person_26' contains 13 unique photos.\n",
      "  -> Folder 'Person_27' contains 2 unique photos.\n",
      "  -> Folder 'Person_28' contains 12 unique photos.\n",
      "  -> Folder 'Person_29' contains 27 unique photos.\n",
      "  -> Folder 'Person_30' contains 3 unique photos.\n",
      "  -> Folder 'Person_31' contains 26 unique photos.\n",
      "  -> Folder 'Person_32' contains 5 unique photos.\n",
      "  -> Folder 'Person_33' contains 2 unique photos.\n",
      "  -> Folder 'Person_34' contains 15 unique photos.\n",
      "  -> Folder 'Person_35' contains 2 unique photos.\n",
      "  -> Folder 'Person_36' contains 6 unique photos.\n",
      "  -> Folder 'Person_38' contains 2 unique photos.\n",
      "  -> Folder 'Person_39' contains 5 unique photos.\n",
      "  -> Folder 'Person_42' contains 4 unique photos.\n",
      "  -> Folder 'Person_43' contains 2 unique photos.\n",
      "  -> Folder 'Person_44' contains 4 unique photos.\n",
      "  -> Folder 'Person_45' contains 3 unique photos.\n",
      "  -> Folder 'Person_46' contains 2 unique photos.\n",
      "  -> Folder 'Person_47' contains 2 unique photos.\n",
      "  -> Folder 'Person_48' contains 9 unique photos.\n",
      "  -> Folder 'Person_49' contains 7 unique photos.\n",
      "  -> Folder 'Person_50' contains 13 unique photos.\n",
      "  -> Folder 'Person_51' contains 7 unique photos.\n",
      "  -> Folder 'Person_53' contains 2 unique photos.\n",
      "  -> Folder 'Person_56' contains 2 unique photos.\n",
      "  -> Folder 'Person_57' contains 2 unique photos.\n",
      "  -> Folder 'Person_58' contains 5 unique photos.\n",
      "  -> Folder 'Person_59' contains 4 unique photos.\n",
      "  -> Folder 'Person_60' contains 5 unique photos.\n",
      "  -> Folder 'Person_61' contains 12 unique photos.\n",
      "  -> Folder 'Person_62' contains 11 unique photos.\n",
      "  -> Folder 'Person_64' contains 4 unique photos.\n",
      "  -> Folder 'Person_65' contains 2 unique photos.\n",
      "  -> Folder 'Person_66' contains 5 unique photos.\n",
      "  -> Folder 'Person_68' contains 2 unique photos.\n",
      "  -> Folder 'Person_69' contains 2 unique photos.\n",
      "  -> Folder 'Person_70' contains 2 unique photos.\n",
      "  -> Folder 'Person_71' contains 2 unique photos.\n",
      "  -> Folder 'Person_72' contains 3 unique photos.\n",
      "  -> Folder 'Person_73' contains 2 unique photos.\n",
      "  -> Folder 'Person_74' contains 4 unique photos.\n",
      "  -> Folder 'Person_75' contains 5 unique photos.\n",
      "  -> Folder 'Person_76' contains 4 unique photos.\n",
      "  -> Folder 'Person_77' contains 2 unique photos.\n",
      "  -> Folder 'Person_78' contains 3 unique photos.\n",
      "  -> Folder 'Person_79' contains 2 unique photos.\n",
      "  -> Folder 'Person_80' contains 2 unique photos.\n",
      "  -> Folder 'Person_81' contains 3 unique photos.\n",
      "  -> Folder 'Person_83' contains 2 unique photos.\n",
      "  -> Folder 'Person_84' contains 2 unique photos.\n",
      "  -> Folder 'Person_85' contains 2 unique photos.\n",
      "  -> Folder 'Person_86' contains 2 unique photos.\n",
      "  -> Folder 'Person_87' contains 2 unique photos.\n",
      "  -> Folder 'Person_88' contains 2 unique photos.\n",
      "\n",
      "âœ… Separation complete!\n",
      "Results are saved in the 'moyodin_resized_Facenet512' directory.\n"
     ]
    }
   ],
   "source": [
    "cluster_and_sort_photos_with_human_feedback(all_face_data, eps=epsilon, buffer=epsilon/100*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04997259",
   "metadata": {},
   "source": [
    "### Below I am using, ITML stands for Information-Theoretic Metric Learning. and it does not work well for face recognition purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "34f5abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import random\n",
    "import cv2 as cv\n",
    "\n",
    "def get_uncertain_pairs(face_data, epsilon=0.25, buffer=0.05, max_questions=None):\n",
    "    \"\"\"\n",
    "    Finds pairs whose distance is within [epsilon - buffer, epsilon + buffer].\n",
    "    These are the 'hard' cases for the AI.\n",
    "    \"\"\"\n",
    "    embeddings = np.array([d['embedding'] for d in face_data])\n",
    "    # Compute the full distance matrix (Cosine is best for faces)\n",
    "    dist_matrix = pairwise_distances(embeddings, metric='cosine')\n",
    "    \n",
    "    # Find indices of pairs within the 'Uncertainty Zone'\n",
    "    uncertain_indices = np.where((dist_matrix > (epsilon - buffer)) & \n",
    "                                 (dist_matrix < (epsilon + buffer)))\n",
    "    \n",
    "    # Filter to avoid self-matches and duplicates (i < j)\n",
    "    candidate_pairs = [(i, j) for i, j in zip(*uncertain_indices) if i < j]\n",
    "    \n",
    "    random.shuffle(candidate_pairs)\n",
    "    return candidate_pairs[:max_questions] if max_questions is not None else candidate_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e01d1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_human_ground_truth(face_data, pairs):\n",
    "    must_link = []\n",
    "    cannot_link = []\n",
    "\n",
    "    idx = 1\n",
    "    for i, j in pairs:\n",
    "        print(f\"\\rQuestion {idx}/{len(pairs)}\", end=\"\")\n",
    "        path1, path2 = face_data[i]['image_path'], face_data[j]['image_path']\n",
    "        loc1, loc2 = face_data[i]['face_location'], face_data[j]['face_location']\n",
    "        \n",
    "        # Display side-by-side\n",
    "        img1 = cv.imread(path1)\n",
    "        img2 = cv.imread(path2)\n",
    "        # faces are located at (x, y, w, h)\n",
    "        x1, y1, w1, h1 = loc1\n",
    "        x2, y2, w2, h2 = loc2\n",
    "        # cv.rectangle(img1, (x1, y1), (x1+w1, y1+h1), (0, 255, 0), 2)\n",
    "        # cv.rectangle(img2, (x2, y2), (x2+w2, y2+h2), (0, 255, 0), 2)\n",
    "        face1 = img1[y1:y1+h1, x1:x1+w1]\n",
    "        face2 = img2[y2:y2+h2, x2:x2+w2]\n",
    "        \n",
    "        if h1 < h2:\n",
    "            face1 = cv.resize(face1, (int(w1 * h2 / h1), h2))\n",
    "        else:\n",
    "            face2 = cv.resize(face2, (int(w2 * h1 / h2), h1))\n",
    "        # Resize for display\n",
    "        combined = np.hstack((face1, face2))\n",
    "        \n",
    "        cv.imshow(\"Same Person? (y=Yes, n=No, q=Quit)\", combined)\n",
    "        key = cv.waitKey(0) & 0xFF\n",
    "        cv.destroyAllWindows()\n",
    "\n",
    "        if key == ord('y'):\n",
    "            must_link.append((i, j))\n",
    "        elif key == ord('n'):\n",
    "            cannot_link.append((i, j))\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "        \n",
    "        idx += 1\n",
    "            \n",
    "    return must_link, cannot_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "013400ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 50 uncertain pairs for human labeling.\n"
     ]
    }
   ],
   "source": [
    "uncertain_pairs = get_uncertain_pairs(all_face_data, epsilon=epsilon, buffer=epsilon/100*1, max_questions=50)\n",
    "print(f\"Collected {len(uncertain_pairs)} uncertain pairs for human labeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ba99ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 50/50"
     ]
    }
   ],
   "source": [
    "must_link, cannot_link = collect_human_ground_truth(face_data=all_face_data, pairs=uncertain_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a9def91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric_learn import ITML\n",
    "\n",
    "# 1. Separate the indices from the labels\n",
    "# metric-learn ITML.fit(pairs, y) expects:\n",
    "# pairs: (n_constraints, 2) array of indices\n",
    "# y: (n_constraints,) array of 1 (same) or -1 (different)\n",
    "pairs = []\n",
    "y = []\n",
    "\n",
    "for i, j in must_link:\n",
    "    pairs.append((i, j))\n",
    "    y.append(1)\n",
    "\n",
    "for i, j in cannot_link:\n",
    "    pairs.append((i, j))\n",
    "    y.append(-1)\n",
    "\n",
    "# 2. Learn the 'New Math' using the base ITML class\n",
    "# We provide the embeddings matrix as a 'preprocessor' so the \n",
    "# model knows how to resolve the indices in 'pairs'.\n",
    "embeddings = np.array([d['embedding'] for d in all_face_data])\n",
    "itml = ITML(preprocessor=embeddings)\n",
    "itml.fit(pairs, y)\n",
    "\n",
    "# 3. Transform your embeddings\n",
    "# This applies the learned Mahalanobis transformation to the data\n",
    "improved_embeddings = itml.transform(embeddings)\n",
    "\n",
    "# 4. Rebuild your data structure\n",
    "improved_all_face_data = []\n",
    "for i, d in enumerate(all_face_data):\n",
    "    improved_all_face_data.append({\n",
    "        'embedding': improved_embeddings[i],\n",
    "        'image_path': d['image_path'],\n",
    "        'face_location': d['face_location']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5598039e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering 719 total face embeddings...\n",
      "\n",
      "ðŸ“¦ Sorting photos into person folders...\n",
      "  -> Folder 'Unknown_Faces' contains 199 unique photos.\n",
      "  -> Folder 'Person_0' contains 3 unique photos.\n",
      "  -> Folder 'Person_1' contains 5 unique photos.\n",
      "  -> Folder 'Person_2' contains 84 unique photos.\n",
      "  -> Folder 'Person_3' contains 6 unique photos.\n",
      "  -> Folder 'Person_4' contains 101 unique photos.\n",
      "  -> Folder 'Person_5' contains 3 unique photos.\n",
      "  -> Folder 'Person_6' contains 7 unique photos.\n",
      "  -> Folder 'Person_7' contains 3 unique photos.\n",
      "  -> Folder 'Person_8' contains 2 unique photos.\n",
      "  -> Folder 'Person_9' contains 2 unique photos.\n",
      "  -> Folder 'Person_10' contains 2 unique photos.\n",
      "  -> Folder 'Person_11' contains 19 unique photos.\n",
      "  -> Folder 'Person_12' contains 2 unique photos.\n",
      "  -> Folder 'Person_13' contains 4 unique photos.\n",
      "  -> Folder 'Person_14' contains 2 unique photos.\n",
      "  -> Folder 'Person_15' contains 2 unique photos.\n",
      "  -> Folder 'Person_16' contains 6 unique photos.\n",
      "  -> Folder 'Person_17' contains 82 unique photos.\n",
      "  -> Folder 'Person_18' contains 5 unique photos.\n",
      "  -> Folder 'Person_19' contains 2 unique photos.\n",
      "  -> Folder 'Person_20' contains 22 unique photos.\n",
      "  -> Folder 'Person_21' contains 2 unique photos.\n",
      "  -> Folder 'Person_22' contains 6 unique photos.\n",
      "  -> Folder 'Person_23' contains 2 unique photos.\n",
      "  -> Folder 'Person_24' contains 7 unique photos.\n",
      "  -> Folder 'Person_25' contains 3 unique photos.\n",
      "  -> Folder 'Person_26' contains 3 unique photos.\n",
      "  -> Folder 'Person_27' contains 3 unique photos.\n",
      "  -> Folder 'Person_28' contains 4 unique photos.\n",
      "  -> Folder 'Person_29' contains 2 unique photos.\n",
      "  -> Folder 'Person_30' contains 2 unique photos.\n",
      "  -> Folder 'Person_31' contains 2 unique photos.\n",
      "  -> Folder 'Person_32' contains 2 unique photos.\n",
      "  -> Folder 'Person_33' contains 2 unique photos.\n",
      "  -> Folder 'Person_34' contains 4 unique photos.\n",
      "  -> Folder 'Person_35' contains 2 unique photos.\n",
      "  -> Folder 'Person_36' contains 12 unique photos.\n",
      "  -> Folder 'Person_37' contains 12 unique photos.\n",
      "  -> Folder 'Person_38' contains 2 unique photos.\n",
      "  -> Folder 'Person_39' contains 2 unique photos.\n",
      "  -> Folder 'Person_40' contains 2 unique photos.\n",
      "  -> Folder 'Person_41' contains 6 unique photos.\n",
      "  -> Folder 'Person_42' contains 3 unique photos.\n",
      "  -> Folder 'Person_43' contains 4 unique photos.\n",
      "  -> Folder 'Person_44' contains 6 unique photos.\n",
      "  -> Folder 'Person_45' contains 3 unique photos.\n",
      "  -> Folder 'Person_46' contains 3 unique photos.\n",
      "  -> Folder 'Person_47' contains 4 unique photos.\n",
      "  -> Folder 'Person_48' contains 8 unique photos.\n",
      "  -> Folder 'Person_49' contains 2 unique photos.\n",
      "  -> Folder 'Person_50' contains 2 unique photos.\n",
      "  -> Folder 'Person_51' contains 2 unique photos.\n",
      "  -> Folder 'Person_52' contains 2 unique photos.\n",
      "  -> Folder 'Person_53' contains 2 unique photos.\n",
      "  -> Folder 'Person_54' contains 2 unique photos.\n",
      "  -> Folder 'Person_55' contains 2 unique photos.\n",
      "  -> Folder 'Person_56' contains 2 unique photos.\n",
      "  -> Folder 'Person_57' contains 4 unique photos.\n",
      "  -> Folder 'Person_58' contains 7 unique photos.\n",
      "  -> Folder 'Person_59' contains 8 unique photos.\n",
      "  -> Folder 'Person_60' contains 2 unique photos.\n",
      "  -> Folder 'Person_61' contains 3 unique photos.\n",
      "  -> Folder 'Person_62' contains 3 unique photos.\n",
      "\n",
      "âœ… Separation complete!\n",
      "Results are saved in the 'jasmin_sorted_Facenet' directory.\n"
     ]
    }
   ],
   "source": [
    "cluster_and_sort_photos(improved_all_face_data, eps=0.15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
